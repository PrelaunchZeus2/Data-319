{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> DATA 319: Model-based and Data-based Methods for Data Analytics. Summer 2024 </h2>\n",
    "<h3> Problem Set 2 </h3>\n",
    "<h3> Team <i> 3 </i></h3>\n",
    "<h3> Type students' names <i> (only those who contributed to the group work)</i> here</h3>\n",
    "- Kikzely Avalos\n",
    "- James Mello\n",
    "- Benjamin Prior\n",
    "- John Salmon\n",
    "- Harpreet Tiwana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. In your own words, explain the curse of dimensionality and its importance in modern data analytics.\n",
    "The curse of dimensionality is not necessarily one thing. The curse of dimenstionality occurs when you have large amounts of data that have varying dimensions. One of the ways to think about this would be diagnosing diseases to patients in healthcare. Each patient has theyre own \"dimensions\" such as age, gender, blood type, history of disease in their family among many other variables, so the curse of dimensionality is in play here because theres so many different dimensions to take into consideration, you cannott treat every patient the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Consider five distance metrics we learned in class. Feel free to refer to [Chapter 3 of MMDS textbook](http://infolab.stanford.edu/~ullman/mmds/ch3n.pdf) (paragraph 3.5. Distance Measures).\n",
    "\n",
    "##### (a) In your own words, provide brief definitions of these 5 distance metrics.\n",
    "\n",
    "Euclidean Distance: This distance is probably the most simple, at its base form, its the distance between 2 points in a plane. Simply it would be taking a ruler and seeing how far apart 2 points are.\n",
    "\n",
    "Jaccard Distance: This one is the measure of how different two sets are. This can be defined as 1 - Jaccard index. The Jacard index can be defined as the # of elements in the sets and B divided by the # of elements in the union of sets A and B. \n",
    "\n",
    "Cosine Distance: This is the dissimilarity of two vectors. This is similar to the Jaccard distance in that its defined as 1 - cosine simiarity which is the dot product of two vectors A and B divided by the product of the magnitude of these vectors. \n",
    "\n",
    "Edit Distance: Edit distance finds how different two strings are by finding how many \"edits\" it takes to transform string 1 into string 2. Edits can include insertions, substitutions or deletions. \n",
    "\n",
    "Hamming Distance: Hamming distance finds the amount of characters in 2 equally sized strings that are different at the same respective indexes. This compares index 1 from string 1 to index 1 in string 2 together to see if they are the same and so on and so forth for the length of the strings. This can only be used when the lengths of the strings are the same. \n",
    "\n",
    "##### (b) Choose two different distance metrics that can be applied to pairs of vectors in $\\mathbb{R}^n$ and explain their differences. For each metric, give an example of a dataset that would be more appropriately analyzed using that metric.\n",
    "\n",
    "The Jaccard Distance measures how similar or dissimilar two vectors are. Its best used in applications where comparing information is important such as survey data containing boolean true or false responses, or comparing how similar two different sets of text are.\n",
    "\n",
    "The Hamming distance finds the number of substitutions required to turn one string of characters into another. It is best used with comparisons of strings such as comparing data stored in computer memory or categorical data such as comparing two users ratings of a movie.\n",
    "\n",
    "While both of these distance metrics enable the comparison of two vectors, the main difference is that the Jaccard Distance doesn't really examine the contents of the sets just the count of the unique combinations. While the Hamming Distance does directly work with the contents of the vectors, transforming them into one another and counting the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### 3. Consider the matrix $M = \\begin{bmatrix} 5 & 1 & 2\\\\ 1 & 3 & 4\\\\ 2 & 4 & 8 \\end{bmatrix}$\n",
    "\n",
    "##### (a) Write down the quadratic form $Q_M (\\mathbf{x})$, where $\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} $\n",
    "5x1^2​+1⋅x1​x2​+2⋅x1​x3​+1⋅x2​x1​+3x2^2​+4⋅x2​x3​+2⋅x3​x1​+4⋅x3​x2​+8x3^2​\n",
    "\n",
    "Quadratic form Q_M(x)= 5x1^2 + 3x2^2 + 8x3^2 + 2x1x2 + 4x1x3 + 8x2x3\n",
    "\n",
    "##### (b) Compute $Q_M\\left( \\begin{bmatrix} 9\\\\ 8\\\\ 7\\end{bmatrix}\\right)$\n",
    "QM([9,8,7]) = 1833\n",
    "\n",
    "##### (c) Compute the distance between $x = \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}$ and $y = \\begin{bmatrix}-7\\\\-8\\\\-9\\end{bmatrix}$ using the distance from $Q_M$\n",
    "x-y = [123] - [-7,-8,-9] = [1+7,2+8,3+9] = [8,10,12]\n",
    "QM([8,10,12]) = [5*8+1*10+2*12,1*8+3*10+4*12,2*8+4*10+8*12] = [74,86,152]\n",
    "([8,10,12])TM[8,10,12] = 8⋅74+10⋅86+12⋅152=592+860+1824=3276\n",
    "Square root of 3276 = 57.14\n",
    "Distance is 57.14\n",
    "##### (d) What is the maximum value of $v^t M v$ subject to $||v|| = 1$ and what $v$ attains that value?\n",
    "The maximum value is 11.043 which is the largest eigenvalue and the vector that attains that value is [0.34679687 0.45187583 0.8219125 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Suppose there are 100 items, numbered 1 to 100, and also 100 baskets, also numbered 1 to 100. Item *i* is in basket *b* if and only if *b* divides *i* with no remainder. Thus, basket 1 will have all 100 items, basket 2 will have all fifty of the even-numbered items, and so on. Basket 12 consists of items $\\{12, 24, 36, 48, 60, 72, 84, 96\\}$, since 12 is a factor of all these numbers. Answer the following questions:\n",
    "\n",
    "##### (a) If the support threshold is 5, which items are frequent?\n",
    "*[12, 16, 18, 20, 24, 28, 30, 32, 36, 40, 42, 44, 45, 48, 50, 52, 54, 56, 60, 63, 64, 66, 68, 70, 72, 75, 76, 78, 80, 81, 84, 88, 90, 92, 96, 98, 99, 100]*\n",
    "\n",
    "##### (b) What is the confidence of the following association rules?\n",
    "i. $\\{24, 60\\}\\rightarrow8$ = 0.5\n",
    " \n",
    "ii. $\\{2, 3, 4\\}\\rightarrow5$ = 1.0\n",
    " \n",
    "\n",
    "##### (c) Apply the A-Priori Algorithm with support threshold 5 to this data.\n",
    "*See Code Below*\n",
    "\n",
    "*Some comments for this question:*\n",
    "- *I uploaded the solution for a question that is similar to (a) on the PS2 assignment page on Canvas, you might find this solution (Python code) useful. Feel free to modify the code and use it to find the answers for these questions.*\n",
    "- *For (c) please find the solution up to frequent quintuplets (5-tuples) using Python. The computational powers of my laptop does not allow me to use Python code to find frequent sets of a higher order, so feel free to find them analytically (or modifying your Python code). Hint: there are 29 frequent 6-tuples, 8 frequent 7-tuples, and one frequent 8-tuple.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4a:\n",
      "Frequent Items: [12, 16, 18, 20, 24, 28, 30, 32, 36, 40, 42, 44, 45, 48, 50, 52, 54, 56, 60, 63, 64, 66, 68, 70, 72, 75, 76, 78, 80, 81, 84, 88, 90, 92, 96, 98, 99, 100]\n",
      "\n",
      "\n",
      "Question 4b part i:\n",
      "Confidence of [24,60] -> 8: 0.5\n",
      "\n",
      "\n",
      "Question 4b part ii:\n",
      "Confidence of [2, 3, 4] -> 5: 1.0\n",
      "\n",
      "\n",
      "Question 4c:\n",
      "     index  support                          itemsets  length\n",
      "0      302     0.06              (36, 12, 48, 24, 60)       5\n",
      "1      303     0.06              (36, 72, 12, 48, 24)       5\n",
      "2      304     0.06              (36, 12, 48, 84, 24)       5\n",
      "3      305     0.06              (96, 36, 12, 48, 24)       5\n",
      "4      306     0.06              (36, 72, 12, 24, 60)       5\n",
      "5      307     0.06              (36, 12, 84, 24, 60)       5\n",
      "6      308     0.06              (96, 36, 12, 24, 60)       5\n",
      "7      309     0.06              (36, 72, 12, 84, 24)       5\n",
      "8      310     0.06              (96, 36, 72, 12, 24)       5\n",
      "9      311     0.06              (96, 36, 12, 84, 24)       5\n",
      "10     312     0.06              (72, 12, 48, 24, 60)       5\n",
      "11     313     0.06              (12, 48, 84, 24, 60)       5\n",
      "12     314     0.06              (96, 12, 48, 24, 60)       5\n",
      "13     315     0.06              (72, 12, 48, 84, 24)       5\n",
      "14     316     0.06              (96, 72, 12, 48, 24)       5\n",
      "15     317     0.06              (96, 12, 48, 84, 24)       5\n",
      "16     318     0.06              (72, 12, 84, 24, 60)       5\n",
      "17     319     0.06              (96, 72, 12, 24, 60)       5\n",
      "18     320     0.06              (96, 12, 84, 24, 60)       5\n",
      "19     321     0.06              (96, 72, 12, 84, 24)       5\n",
      "20     322     0.06              (36, 72, 12, 48, 60)       5\n",
      "21     323     0.06              (36, 12, 48, 84, 60)       5\n",
      "22     324     0.06              (96, 36, 12, 48, 60)       5\n",
      "23     325     0.06              (36, 72, 12, 48, 84)       5\n",
      "24     326     0.06              (96, 36, 72, 12, 48)       5\n",
      "25     327     0.06              (96, 36, 12, 48, 84)       5\n",
      "26     328     0.06              (36, 72, 12, 84, 60)       5\n",
      "27     329     0.06              (96, 36, 72, 12, 60)       5\n",
      "28     330     0.06              (96, 36, 12, 84, 60)       5\n",
      "29     331     0.06              (96, 36, 72, 12, 84)       5\n",
      "30     332     0.06              (72, 12, 48, 84, 60)       5\n",
      "31     333     0.06              (96, 72, 12, 48, 60)       5\n",
      "32     334     0.06              (96, 12, 48, 84, 60)       5\n",
      "33     335     0.06              (96, 72, 12, 48, 84)       5\n",
      "34     336     0.06              (96, 72, 12, 84, 60)       5\n",
      "35     337     0.05              (32, 64, 48, 16, 80)       5\n",
      "36     338     0.05              (32, 64, 96, 48, 16)       5\n",
      "37     339     0.05              (32, 96, 48, 16, 80)       5\n",
      "38     340     0.05              (32, 64, 96, 80, 16)       5\n",
      "39     341     0.05              (96, 64, 48, 16, 80)       5\n",
      "40     342     0.06              (36, 72, 18, 54, 90)       5\n",
      "41     343     0.06             (100, 40, 80, 20, 60)       5\n",
      "42     344     0.06              (36, 72, 48, 24, 60)       5\n",
      "43     345     0.06              (36, 48, 84, 24, 60)       5\n",
      "44     346     0.06              (96, 36, 48, 24, 60)       5\n",
      "45     347     0.06              (36, 72, 48, 84, 24)       5\n",
      "46     348     0.06              (96, 36, 72, 48, 24)       5\n",
      "47     349     0.06              (96, 36, 48, 84, 24)       5\n",
      "48     350     0.06              (36, 72, 84, 24, 60)       5\n",
      "49     351     0.06              (96, 36, 72, 24, 60)       5\n",
      "50     352     0.06              (96, 36, 84, 24, 60)       5\n",
      "51     353     0.06              (96, 36, 72, 84, 24)       5\n",
      "52     354     0.06              (72, 48, 84, 24, 60)       5\n",
      "53     355     0.06              (96, 72, 48, 24, 60)       5\n",
      "54     356     0.06              (96, 48, 84, 24, 60)       5\n",
      "55     357     0.06              (96, 72, 48, 84, 24)       5\n",
      "56     358     0.06              (96, 72, 84, 24, 60)       5\n",
      "57     359     0.05              (64, 32, 96, 48, 80)       5\n",
      "58     360     0.06              (36, 72, 48, 84, 60)       5\n",
      "59     361     0.06              (96, 36, 72, 48, 60)       5\n",
      "60     362     0.06              (96, 36, 48, 84, 60)       5\n",
      "61     363     0.06              (96, 36, 72, 48, 84)       5\n",
      "62     364     0.06              (96, 36, 72, 84, 60)       5\n",
      "63     365     0.06              (96, 72, 48, 84, 60)       5\n",
      "64     366     0.06          (36, 72, 12, 48, 24, 60)       6\n",
      "65     367     0.06          (36, 12, 48, 84, 24, 60)       6\n",
      "66     368     0.06          (96, 36, 12, 48, 24, 60)       6\n",
      "67     369     0.06          (36, 72, 12, 48, 84, 24)       6\n",
      "68     370     0.06          (96, 36, 72, 12, 48, 24)       6\n",
      "69     371     0.06          (96, 36, 12, 48, 84, 24)       6\n",
      "70     372     0.06          (36, 72, 12, 84, 24, 60)       6\n",
      "71     373     0.06          (96, 36, 72, 12, 24, 60)       6\n",
      "72     374     0.06          (96, 36, 12, 84, 24, 60)       6\n",
      "73     375     0.06          (96, 36, 72, 12, 84, 24)       6\n",
      "74     376     0.06          (72, 12, 48, 84, 24, 60)       6\n",
      "75     377     0.06          (96, 72, 12, 48, 24, 60)       6\n",
      "76     378     0.06          (96, 12, 48, 84, 24, 60)       6\n",
      "77     379     0.06          (96, 72, 12, 48, 84, 24)       6\n",
      "78     380     0.06          (96, 72, 12, 84, 24, 60)       6\n",
      "79     381     0.06          (36, 72, 12, 48, 84, 60)       6\n",
      "80     382     0.06          (96, 36, 72, 12, 48, 60)       6\n",
      "81     383     0.06          (96, 36, 12, 48, 84, 60)       6\n",
      "82     384     0.06          (96, 36, 72, 12, 48, 84)       6\n",
      "83     385     0.06          (96, 36, 72, 12, 84, 60)       6\n",
      "84     386     0.06          (96, 72, 12, 48, 84, 60)       6\n",
      "85     387     0.05          (32, 96, 64, 48, 16, 80)       6\n",
      "86     388     0.06          (36, 72, 48, 84, 24, 60)       6\n",
      "87     389     0.06          (96, 36, 72, 48, 24, 60)       6\n",
      "88     390     0.06          (96, 36, 48, 84, 24, 60)       6\n",
      "89     391     0.06          (96, 36, 72, 48, 84, 24)       6\n",
      "90     392     0.06          (96, 36, 72, 84, 24, 60)       6\n",
      "91     393     0.06          (96, 72, 48, 84, 24, 60)       6\n",
      "92     394     0.06          (96, 36, 72, 48, 84, 60)       6\n",
      "93     395     0.06      (36, 72, 12, 48, 84, 24, 60)       7\n",
      "94     396     0.06      (96, 36, 72, 12, 48, 24, 60)       7\n",
      "95     397     0.06      (96, 36, 12, 48, 84, 24, 60)       7\n",
      "96     398     0.06      (96, 36, 72, 12, 48, 84, 24)       7\n",
      "97     399     0.06      (96, 36, 72, 12, 84, 24, 60)       7\n",
      "98     400     0.06      (96, 72, 12, 48, 84, 24, 60)       7\n",
      "99     401     0.06      (96, 36, 72, 12, 48, 84, 60)       7\n",
      "100    402     0.06      (96, 36, 72, 48, 84, 24, 60)       7\n",
      "101    403     0.06  (96, 36, 72, 12, 48, 84, 24, 60)       8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code for Question 4a:\n",
    "from collections import Counter\n",
    "baskets = [[i for i in range(1,101) if i % b == 0] for b in range(1,101)]\n",
    "flat_basket = [item for basket in baskets for item in basket]\n",
    "item_counts = Counter(flat_basket)\n",
    "frequent_itemsets = [item for item, count in item_counts.items() if count >=5]\n",
    "print(f\"Question 4a:\\nFrequent Items: {frequent_itemsets}\\n\\n\")\n",
    "\n",
    "# Code for Question 4b part i:\n",
    "ant = set([24, 60])\n",
    "con = set([8])\n",
    "ant_count = 0\n",
    "both_count = 0\n",
    "for basket in baskets:\n",
    "    if ant.issubset(basket):\n",
    "        ant_count += 1\n",
    "        if con.issubset(basket):\n",
    "            both_count += 1\n",
    "\n",
    "confidence = both_count / ant_count\n",
    "print(f\"Question 4b part i:\\nConfidence of [24,60] -> 8: {confidence}\\n\\n\")\n",
    "\n",
    "# Code for Question 4b part ii:\n",
    "ant = [2, 3, 4]\n",
    "con = [5]\n",
    "ant_count = 0\n",
    "both_count = 0\n",
    "for basket in baskets:\n",
    "    if set(ant).issubset(basket):\n",
    "        ant_count += 1\n",
    "        if set(con).issubset(basket):\n",
    "            both_count += 1\n",
    "\n",
    "confidence = both_count / ant_count\n",
    "print(f\"Question 4b part ii:\\nConfidence of [2, 3, 4] -> 5: {confidence}\\n\\n\")\n",
    "\n",
    "# Code for Question 4c:\n",
    "try:\n",
    "    import mlxtend, pandas\n",
    "except ModuleNotFoundError:\n",
    "    %pip -q install pandas mlxtend\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "baskets = [[i for i in range(1,101) if i % b == 0] for b in range(1,101)]\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(baskets).transform(baskets)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "priori_sets = apriori(df, min_support = 0.05, use_colnames = True)\n",
    "priori_sets['length'] = priori_sets['itemsets'].apply(lambda x: len(x))\n",
    "priori_sets = priori_sets[priori_sets['length'] >= 5].reset_index(drop = False)\n",
    "print(f\"Question 4c:\\n{priori_sets}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Imagine we have summarized a collection of documents and words with the following table:\n",
    "\n",
    "|  | zoo | kangaroo | monkey | alligator | tiger | camel | eagle | lemur | dragon | pizza |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| **doc 1** | 51 | 92 | 14 | 71 | 60 | 20 | 82 | 86 | 74 | 74 |\n",
    "| **doc 2** | 87 | 99 | 23 | 2  | 21 | 52 | 1  | 87 | 29 | 37 |\n",
    "| **doc 3** | 1  | 63 | 59 | 20 | 32 | 75 | 57 | 21 | 88 | 48 |\n",
    "| **doc 4** | 90 | 58 | 41 | 91 | 59 | 79 | 14 | 61 | 61 | 46 |\n",
    "| **doc 5** | 61 | 50 | 54 | 63 | 2  | 50 | 6  | 20 | 72 | 38 |\n",
    "| **doc 6** | 17 | 3  | 88 | 59 | 13 | 8  | 89 | 52 | 1  | 83 |\n",
    "| **doc 7** | 91 | 59 | 70 | 43 | 7  | 46 | 34 | 77 | 80 | 35 |\n",
    "| **doc 8** | 49 | 3  | 1  | 5  | 53 | 3  | 53 | 92 | 62 | 17 |\n",
    "| **doc 9** | 89 | 43 | 33 | 73 | 61 | 99 | 13 | 94 | 47 | 14 |\n",
    "| **doc 10**| 71 | 77 | 86 | 61 | 39 | 84 | 79 | 81 | 52 | 23 |\n",
    "| **doc 11**| 25 | 88 | 59 | 40 | 28 | 14 | 44 | 64 | 88 | 70 |\n",
    "| **doc 12**| 8  | 87 | 0  | 7  | 87 | 62 | 10 | 80 | 7  | 34 |\n",
    "| **doc 13**| 34 | 32 | 4  | 40 | 27 | 6  | 72 | 71 | 11 | 33 |\n",
    "| **doc 14**| 32 | 47 | 22 | 61 | 87 | 36 | 98 | 43 | 85 | 90 |\n",
    "| **doc 15**| 34 | 64 | 98 | 46 | 77 | 2  | 0  | 4  | 89 | 13 |\n",
    "\n",
    "##### (a) Compute the reduced SVD embedding with 2 dimensions for all 15 documents and make a scatterplot of the reduced points.\n",
    "See Below\n",
    "\n",
    "##### (b) Compute the reduced SVD embedding with 2 dimensions for all 10 words and make a scatterplot of the reduced points.\n",
    "See Below\n",
    "\n",
    "##### (c) What patterns do you observe from these 2-d embeddings?\n",
    "The patterns in the scatterplots shows that there are several clustering of groups. In the documents scatterplot, one cluster is made up of Doc 3, 15, 5, 12, 8, and 13. Another cluster is Doc 7, 4, and 9. This could mean that these groups of documents might have similar elements such as concepts. In the words scatterplot, there are two main clusters. One cluster includes the words eagle, pizza, monkey, tiger, and alligator. The other cluster has dragon, lemur, and kangaroo. This could mean that these groups of words could have common attributes or ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# making table into a matrix\n",
    "table = np.array([\n",
    "51, 92, 14, 71, 60, 20, 82, 86, 74, 74,\n",
    "87, 99, 23, 2, 21, 52, 1, 87, 29, 37,\n",
    "1, 63, 59, 20, 32, 75, 57, 21, 88, 48,\n",
    "90, 58, 41, 91, 59, 79, 14, 61, 61, 46,\n",
    "61, 50, 54, 63, 2, 50, 6, 20, 72, 38,\n",
    "17, 3, 88, 59, 13, 8, 89, 52, 1, 83,\n",
    "91, 59, 70, 43, 7, 46, 34, 77, 80, 35,\n",
    "49, 3, 1, 5, 53, 3, 53, 92, 62, 17,\n",
    "89, 43, 33, 73, 61, 99, 13, 94, 47, 14,\n",
    "71, 77, 86, 61, 39, 84, 79, 81, 52, 23,\n",
    "25, 88, 59, 40, 28, 14, 44, 64, 88, 70,\n",
    "8, 87, 0, 7, 87, 62, 10, 80, 7, 34,\n",
    "34, 32, 4, 40, 27, 6, 72, 71, 11, 33,\n",
    "32, 47, 22, 61, 87, 36, 98, 43, 85, 90,\n",
    "34, 64, 98, 46, 77, 2, 0, 4, 89, 13\n",
    "]).reshape(15, 10)\n",
    "\n",
    "# Part (a): documents, SVD with 2 dimensions\n",
    "a, b, c = LA.svd(table)\n",
    "new_doc = a[:, :2]\n",
    "\n",
    "# Scatterplot for documents\n",
    "plt.scatter(new_doc[:, 0], new_doc[:, 1], color='lightblue')\n",
    "\n",
    "# using loop to add each document to the data points, making it easier to see patterns\n",
    "for i, title in enumerate(range(1, 16)):\n",
    "    plt.annotate(f'Doc {title}', (new_doc[i, 0], new_doc[i, 1]))\n",
    "\n",
    "plt.title('Reduced SVD embedding with 2 dimensions for Documents')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# Part (b): words, SVD with 2 dimensions\n",
    "# transposing table for words\n",
    "new_word = c.T[:, :2]\n",
    "\n",
    "# Scatterplot for words\n",
    "plt.scatter(new_word[:, 0], new_word[:, 1], color='pink')\n",
    "\n",
    "# using loop to add each word to the data points, making it easier to see patterns\n",
    "word_list = [\"zoo\", \"kangaroo\", \"monkey\", \"alligator\", \"tiger\", \"camel\", \"eagle\", \"lemur\", \"dragon\", \"pizza\"]\n",
    "for i, title1 in enumerate(word_list):\n",
    "    plt.annotate(title1, (new_word[i, 0], new_word[i, 1]))\n",
    "\n",
    "plt.title('Reduced SVD embedding with 2 dimensions for Words')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
